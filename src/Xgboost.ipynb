{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from sklearn.svm import SVC\n",
    "from Preprocessing_functions import *\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\inesm\\AppData\\Local\\Temp\\ipykernel_33896\\3470921380.py:1: DtypeWarning: Columns (29) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  train_data = pd.read_csv('train_data.csv', index_col='Claim Identifier')\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_data.csv', index_col='Claim Identifier')\n",
    "test_data = pd.read_csv('test_data.csv', index_col='Claim Identifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and validation sets\n",
    "train_data = train_data[~(train_data.drop(columns=['Assembly Date']).isna().all(axis=1) & train_data['Assembly Date'].notna())] \n",
    "X = train_data.drop(columns=['Claim Injury Type', 'WCB Decision', 'Agreement Reached'])\n",
    "y = train_data['Claim Injury Type']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODE_COLUMNS = ['Industry Code', 'WCIO Cause of Injury Code',\n",
    "       'WCIO Nature of Injury Code', 'WCIO Part Of Body Code']\n",
    "\n",
    "DESCRIPTION_COLUMNS = ['WCIO Cause of Injury Description','WCIO Nature of Injury Description','WCIO Part Of Body Description','Industry Code Description']\n",
    "\n",
    "BOOLEAN_COLUMNS = ['Alternative Dispute Resolution', 'Attorney/Representative','COVID-19 Indicator']\n",
    "\n",
    "date_order = ['Accident Date', 'C-2 Date','C-3 Date','Assembly Date', 'First Hearing Date']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [\n",
    "    'Accident Date', \n",
    "    'Age at Injury', \n",
    "    'Assembly Date', \n",
    "    'Average Weekly Wage', \n",
    "    'Birth Year', \n",
    "    'C-2 Date', \n",
    "    'C-3 Date', \n",
    "    'First Hearing Date', \n",
    "    'IME-4 Count', \n",
    "]\n",
    "\n",
    "categorical_features = ['Alternative Dispute Resolution',\n",
    " 'Attorney/Representative',\n",
    " 'Carrier Name',\n",
    " 'Carrier Type',\n",
    " 'County of Injury',\n",
    " 'COVID-19 Indicator',\n",
    " 'District Name',\n",
    " 'Gender',\n",
    " 'Industry Code',\n",
    " 'Medical Fee Region',\n",
    " 'WCIO Cause of Injury Code',\n",
    " 'WCIO Nature of Injury Code',\n",
    " 'WCIO Part Of Body Code',\n",
    " 'Zip Code']\n",
    "\n",
    "col_minmax = ['Age at Injury',\n",
    "               'Birth Year', \n",
    "               'Number of Dependents']\n",
    "\n",
    "col_standart = ['Accident Date',\n",
    "                'Assembly Date',\n",
    "                'Average Weekly Wage',\n",
    "                ]\n",
    "\n",
    "low_cardinality_cols = [col for col in categorical_features if X_train[col].nunique() < 10]\n",
    "high_cardinality_cols = [col for col in categorical_features if X_train[col].nunique() > 10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Carrier Name',\n",
       " 'County of Injury',\n",
       " 'Industry Code',\n",
       " 'WCIO Cause of Injury Code',\n",
       " 'WCIO Nature of Injury Code',\n",
       " 'WCIO Part Of Body Code',\n",
       " 'Zip Code']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_cardinality_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[categorical_features] = X_train[categorical_features].astype(str)\n",
    "X_val[categorical_features] = X_val[categorical_features].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_description_columns(X_train, X_val):\n",
    "    \"\"\"\n",
    "    Drop all columns in X_train and X_val that contain the word 'description' in their names (case-insensitive).\n",
    "    \"\"\"\n",
    "    description_columns = X_train.columns[X_train.columns.str.contains('description', case=False, na=False)]\n",
    "    \n",
    "\n",
    "    X_train = X_train.drop(description_columns, axis=1)\n",
    "    X_val = X_val.drop(description_columns, axis=1)\n",
    "    \n",
    "    return X_train, X_val\n",
    "\n",
    "X_train ,X_val = drop_description_columns(X_train, X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_description_columns_Test(X_test):\n",
    "    \"\"\"\n",
    "    Drop all columns in X_train and X_val that contain the word 'description' in their names (case-insensitive).\n",
    "    \"\"\"\n",
    "    description_columns = X_test.columns[X_test.columns.str.contains('description', case=False, na=False)]\n",
    "    \n",
    "\n",
    "    X_test = X_test.drop(description_columns, axis=1)\n",
    "    \n",
    "    return X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_dum(X_train, X_val):\n",
    "    drop_description_columns(X_train, X_val)\n",
    "    convert_to_timestamp(X_train, X_val, date_order)\n",
    "    convert_to_bool(X_train, X_val, col_names=BOOLEAN_COLUMNS)\n",
    "    impute_mean_numerical(X_train, X_val, numerical_columns)\n",
    "    fill_missing_with_mode(X_train, X_val)\n",
    "    feature_creation_has_Cdate(X_train, X_val)\n",
    "    # columns_to_drop = ['C-2 Date', 'C-3 Date', 'First Hearing Date']\n",
    "    # X_train = X_train.drop(columns=columns_to_drop)\n",
    "    # X_val = X_val.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "    return X_train, X_val\n",
    "\n",
    "def preprocessing_dum_test(X_test):\n",
    "    convert_to_timestamp_test(X_test, date_order)\n",
    "    convert_to_bool_test(X_test, col_names=BOOLEAN_COLUMNS)\n",
    "    impute_mean_numerical_test(X_test, numerical_columns)\n",
    "    fill_missing_with_mode_test(X_test)\n",
    "    feature_creation_has_Cdate_test(X_test)\n",
    "\n",
    "def scaling_encoding(X_train, X_val):\n",
    "    scaling_minmax(X_train, X_val, col_minmax)\n",
    "    scaling_standard(X_train, X_val, col_standart)\n",
    "    X_train, X_val = encoding_onehot(X_train, X_val, low_cardinality_cols)\n",
    "    X_train, X_val = encoding_frequency1(X_train, X_val, high_cardinality_cols)\n",
    "\n",
    "\n",
    "    return X_train, X_val\n",
    "\n",
    "def scaling_encoding_test(X_test):\n",
    "    scaling_minmax_test(X_test, col_minmax)\n",
    "    scaling_standard_test(X_test, col_standart)\n",
    "    X_test= encoding_onehot_test(X_test, low_cardinality_cols)\n",
    "    X_test = encoding_frequency1_test(X_test, high_cardinality_cols)\n",
    "\n",
    "\n",
    "    return X_train, X_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocessing(X_train, X_val):\n",
    "\n",
    "#     X_train=X_train.drop_duplicates()\n",
    "#     X_val=X_val.drop_duplicates()\n",
    "\n",
    "#     convert_to_bool(X_train,X_val)\n",
    "#     #convert to timestamp all columns with Date included in the name\n",
    "#     #convert_to_timestamp(X_train,X_val,xxxx)\n",
    "#     #convert_to_datetime(X_train,X_val,xxx)\n",
    "\n",
    "#     X_train = X_train.drop(columns=['OIICS Nature of Injury Description'])\n",
    "#     X_val = X_val.drop(columns=['OIICS Nature of Injury Description'])\n",
    "\n",
    "#     impute_mean_numerical(X_train, X_val, ['C-2 Date'])\n",
    "#     fillnan_accident_date(X_train,X_val)\n",
    "#     fillnan_birth_year(X_train,X_val)\n",
    "\n",
    "#     #drop average weekly wage bc lots of 0 bc fuck it\n",
    "#     X_train = X_train.drop(columns=['Average Weekly Wage'])\n",
    "#     X_val = X_val.drop(columns=['Average Weekly Wage'])\n",
    "\n",
    "#     #fill IME4 Count with 0s \n",
    "#     X_train[\"IME-4 Count\"].fillna(0, inplace=True)\n",
    "#     X_val[\"IME-4 Count\"].fillna(0, inplace=True)\n",
    "\n",
    "#     X_train['Has First Hearing Date'] = X_train['First Hearing Date'].apply(lambda x: 0 if x == np.nan else 1)\n",
    "#     X_val['Has First Hearing Date'] = X_val['First Hearing Date'].apply(lambda x: 0 if x == np.nan else 1)\n",
    "\n",
    "#     # drop first hearing date\n",
    "#     X_train = X_train.drop(columns=['First Hearing Date'])\n",
    "#     X_val = X_val.drop(columns=['First Hearing Date'])\n",
    "\n",
    "#     # creation of 'Has C2 date'\n",
    "#     X_train['Has C-2 Date'] = X_train['C-2 Date'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "#     X_val['Has C-2 Date'] = X_val['C-2 Date'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "#     impute_mode_categorical(X_train,X_val,['Has C-2 Date'])\n",
    "\n",
    "#     #codes and stuff\n",
    "#     fill_missing_codes_description_based(X_train,X_val)\n",
    "#     fill_missing_with_mode(X_train,X_val)\n",
    "\n",
    "#     #C3 date\n",
    "#     X_train['Has C-3 Date'] = X_train['C-3 Date'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "#     X_val['Has C-3 Date'] = X_val['C-3 Date'].apply(lambda x: 0 if pd.isna(x)  else 1)\n",
    "\n",
    "#     X_train = X_train.drop(columns=['C-3 Date'])\n",
    "#     X_val = X_val.drop(columns=['C-3 Date'])\n",
    "\n",
    "#     impute_mode_categorical(X_train,X_val,['Alternate Dispute Resolution'])\n",
    "\n",
    "#     return X_train, X_val\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import f1_score\n",
    "# from sklearn.model_selection import KFold\n",
    "# from sklearn.feature_selection import RFE\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # Garantir que os índices de X e y estejam alinhados\n",
    "# X = X.reset_index(drop=True)\n",
    "# y = y.reset_index(drop=True)\n",
    "\n",
    "# # Faixa de valores para o parâmetro C\n",
    "# c_range = np.logspace(-3, 2, 10)  # Exemplo de valores de 0.001 a 100\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# mean_f1_scores = []\n",
    "\n",
    "# for c in c_range:\n",
    "#     f1_scores = []\n",
    "#     for train_index, test_index in kf.split(X):\n",
    "#         # Dividir o dataset em treino e validação\n",
    "#         X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
    "#         y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "#         # Pré-processamento\n",
    "#         X_train, X_val = drop_description_columns(X_train, X_val)\n",
    "#         X_train, X_val = preprocessing_dum(X_train, X_val)\n",
    "#         X_train, X_val = scaling_encoding(X_train, X_val)\n",
    "#         y_train,y_val=encoding_label(y_train,y_val)\n",
    "        \n",
    "#         # Seleção de features com RFE\n",
    "#         X_train_selected, selected_features, feature_ranking = feature_selection_rfe(\n",
    "#             X_train, y_train, 10, LogisticRegression()\n",
    "#         )\n",
    "        \n",
    "#         # Treinar o SVM com o valor atual de C\n",
    "#         svm = SVC(C=c, kernel='linear', random_state=42)\n",
    "#         svm.fit(X_train_selected, y_train)\n",
    "        \n",
    "#         # Fazer previsões e calcular o F1 score\n",
    "#         y_pred = svm.predict(X_val[selected_features])\n",
    "#         f1 = f1_score(y_val, y_pred, average='macro')\n",
    "#         f1_scores.append(f1)\n",
    "\n",
    "#     # Armazenar a média dos F1 scores para o valor de C atual\n",
    "#     mean_f1_scores.append(np.mean(f1_scores))\n",
    "\n",
    "# # Determinar o valor ótimo de C\n",
    "# optimal_c = c_range[np.argmax(mean_f1_scores)]\n",
    "# print(f\"The optimal value of C is {optimal_c}.\")\n",
    "\n",
    "# # Plotar os F1 scores médios para cada valor de C\n",
    "# plt.plot(c_range, mean_f1_scores)\n",
    "# plt.xscale('log')  # Escala logarítmica para melhor visualização\n",
    "# plt.xlabel('C (Regularization Parameter)')\n",
    "# plt.ylabel('Mean F1 Score')\n",
    "# plt.title('Optimal C Selection using K-Fold Cross-Validation')\n",
    "# plt.show()\n",
    "\n",
    "# # Treinar o modelo final usando todo o conjunto de dados\n",
    "# X_preprocessed, _ = preprocessing_dum(X, X)\n",
    "# X_scaled, _ = scaling_encoding(X_preprocessed, X_preprocessed)\n",
    "# selector = RFE(estimator=LogisticRegression(), n_features_to_select=10)\n",
    "# X_final = selector.fit_transform(X_scaled, y)\n",
    "# final_svm = SVC(C=optimal_c, kernel='linear', random_state=42)\n",
    "# final_svm.fit(X_final, y)\n",
    "\n",
    "# print(f\"Model trained with optimal C={optimal_c}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.metrics import accuracy_score, f1_score\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # Garantir que os índices de X e y estejam alinhados\n",
    "# X = X.reset_index(drop=True)\n",
    "# y = y.reset_index(drop=True)\n",
    "\n",
    "# # Definir o learning rate único\n",
    "# learning_rate = 0.5  # Você pode alterar este valor conforme necessário\n",
    "\n",
    "# # Pré-processamento\n",
    "# print(\"Realizando o pré-processamento...\")\n",
    "# X_train, X_val = drop_description_columns(X_train, X_val)\n",
    "# X_train, X_val = preprocessing_dum(X_train, X_val)\n",
    "# X_train, X_val = scaling_encoding(X_train, X_val)\n",
    "# y_train, y_val = encoding_label(y_train, y_val)\n",
    "\n",
    "# # Seleção de features com RFE\n",
    "# print(\"Selecionando features com RFE...\")\n",
    "# X_train_selected, selected_features, feature_ranking = feature_selection_rfe(\n",
    "#     X_train, y_train, 35, LogisticRegression()\n",
    "# )\n",
    "\n",
    "# # Treinamento do modelo\n",
    "# print(\"Treinando o modelo...\")\n",
    "# model = XGBClassifier(learning_rate=learning_rate, use_label_encoder=False, eval_metric='mlogloss')\n",
    "# model.fit(X_train_selected, y_train)\n",
    "\n",
    "# # Avaliação no conjunto de validação\n",
    "# print(\"Avaliando no conjunto de validação...\")\n",
    "# X_val_selected = X_val[selected_features]\n",
    "# y_pred_val = model.predict(X_val_selected)\n",
    "# val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "# val_f1 = f1_score(y_val, y_pred_val, average='weighted')  # Use \"weighted\" para classes desbalanceadas\n",
    "\n",
    "# # Resultados\n",
    "# print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "# print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizando o pré-processamento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\inesm\\OneDrive\\Documentos\\GitHub\\ML_Group36\\src\\Preprocessing_functions.py:330: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_train[col].fillna(mean_value, inplace=True)\n",
      "c:\\Users\\inesm\\OneDrive\\Documentos\\GitHub\\ML_Group36\\src\\Preprocessing_functions.py:331: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_val[col].fillna(mean_value, inplace=True)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'convert_to_timestamp_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m X_test \u001b[38;5;241m=\u001b[39m test_data[\u001b[38;5;241m~\u001b[39m(test_data\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAssembly Date\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39mall(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAssembly Date\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnotna())] \n\u001b[0;32m     29\u001b[0m X_test\u001b[38;5;241m=\u001b[39m drop_description_columns_Test(X_test)\n\u001b[1;32m---> 30\u001b[0m X_test\u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessing_dum_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m X_test \u001b[38;5;241m=\u001b[39m scaling_encoding_test(X_test)\n\u001b[0;32m     33\u001b[0m enc2\u001b[38;5;241m=\u001b[39mLabelEncoder()\n",
      "Cell \u001b[1;32mIn[82], line 16\u001b[0m, in \u001b[0;36mpreprocessing_dum_test\u001b[1;34m(X_test)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocessing_dum_test\u001b[39m(X_test):\n\u001b[1;32m---> 16\u001b[0m     \u001b[43mconvert_to_timestamp_test\u001b[49m(X_test, date_order)\n\u001b[0;32m     17\u001b[0m     convert_to_bool_test(X_test, col_names\u001b[38;5;241m=\u001b[39mBOOLEAN_COLUMNS)\n\u001b[0;32m     18\u001b[0m     impute_mean_numerical_test(X_test, numerical_columns)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'convert_to_timestamp_test' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "\n",
    "# Garantir que os índices de X e y estejam alinhados\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "# Definir o learning rate único\n",
    "learning_rate = 0.5  # Você pode alterar este valor conforme necessário\n",
    "\n",
    "# Pré-processamento\n",
    "print(\"Realizando o pré-processamento...\")\n",
    "X_train, X_val = preprocessing_dum(X_train, X_val)\n",
    "X_train, X_val = scaling_encoding(X_train, X_val)\n",
    "X_train,X_val=outliers_iqr(X_train,X_val,X_train.columns)\n",
    "y_train, y_val = encoding_label(y_train, y_val)\n",
    "\n",
    "#preprocessing test data\n",
    "X_test = test_data[~(test_data.drop(columns=['Assembly Date']).isna().all(axis=1) & test_data['Assembly Date'].notna())] \n",
    "X_test= drop_description_columns_Test(X_test)\n",
    "X_test= preprocessing_dum_test(X_test)\n",
    "X_test = scaling_encoding_test(X_test)\n",
    "\n",
    "enc2=LabelEncoder()\n",
    "enc2.fit(y_train)\n",
    "\n",
    "# Seleção de features com RFECV\n",
    "print(\"Selecionando features com RFECV...\")\n",
    "# model_for_rfe = LogisticRegression(max_iter=1000)  # Modelo base para RFECV\n",
    "# cv_strategy = StratifiedKFold(n_splits=5)  # Estratégia de validação cruzada\n",
    "\n",
    "# rfecv = RFECV(estimator=model_for_rfe, step=1, cv=cv_strategy, scoring='accuracy', n_jobs=-1)\n",
    "# rfecv.fit(X_train, y_train)\n",
    "\n",
    "#selected_features = X_train.columns[rfecv.support_]\n",
    "selected_features= X_train.columns['Average Weekly Wage', 'C-2 Date', 'C-3 Date', 'First Hearing Date', 'IME-4 Count', 'Attorney/Representative_False', 'Attorney/Representative_True', 'Carrier Type_1A. PRIVATE', 'Carrier Type_2A. SIF']\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_val_selected = X_val[selected_features]\n",
    "\n",
    "print(f\"Número de features selecionadas: {len(selected_features)}\")\n",
    "print(\"Features selecionadas:\", selected_features.tolist())\n",
    "\n",
    "# Treinamento do modelo com as features selecionadas\n",
    "print(\"Treinando o modelo...\")\n",
    "model = XGBClassifier(learning_rate=learning_rate, use_label_encoder=False, eval_metric='mlogloss')\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Avaliação no conjunto de validação\n",
    "print(\"Avaliando no conjunto de validação...\")\n",
    "y_pred_val = model.predict(X_val_selected)\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_f1 = f1_score(y_val, y_pred_val, average='macro')  \n",
    "y_pred_test = model.predict(X_test[selected_features])\n",
    "\n",
    "# Resultados\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "# # Plotando o número de features vs desempenho\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.title('Número de Features vs Acurácia')\n",
    "# plt.xlabel('Número de Features Selecionadas')\n",
    "# plt.ylabel('Acurácia de Validação Cruzada')\n",
    "# plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.378223164136909"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_f1 = f1_score(y_val, y_pred_val, average='macro')  \n",
    "val_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  warnings.warn(smsg, UserWarning)\n",
    "Número de features selecionadas: 9\n",
    "Features selecionadas: ['Average Weekly Wage', 'C-2 Date', 'C-3 Date', 'First Hearing Date', 'IME-4 Count', 'Attorney/Representative_False', 'Attorney/Representative_True', 'Carrier Type_1A. PRIVATE', 'Carrier Type_2A. SIF']\n",
    "Treinando o modelo...\n",
    "Avaliando no conjunto de validação...\n",
    "Validation Accuracy: 0.7765\n",
    "Validation F1 Score: 0.7295\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['4. TEMPORARY', '4. TEMPORARY', '2. NON-COMP', ..., '3. MED ONLY',\n",
       "       '3. MED ONLY', '3. MED ONLY'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = enc2.inverse_transform(y_pred_test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## formating the submission file\n",
    "X_test['Claim Injury Type'] = test\n",
    "sample_submission = X_test[['Claim Injury Type']].set_index(X_test.index)\n",
    "sample_submission.to_csv('submission_logistic regression_outliers.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2425",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
